首先来看一下给出的wc.go文件

> func Map(filename string, contents string) []mr.KeyValue {
	// function to detect word separators.
	ff := func(r rune) bool { return !unicode.IsLetter(r) }

在Map函数中，首先定义了一个ff的函数用来判断给定一个字符，这个字符到底是不是一个letter，返回布尔值，
ff 函数用于检测单词分隔符，如果 r 不是字母（即 !unicode.IsLetter(r)），则返回 true，表示这是一个分隔符。

> words := strings.FieldsFunc(contents, ff)

在这里，这段代码使用了FieldsFunc函数来分割我们传入的字符串contents，具体判断在哪里分割，则要用到我们
刚才定义的ff函数，即以非字母字符作为分隔符。

注意到，论文中提到，Map函数要返回一个中间值，在这里我们使用定义好的KeyValue类型来标识中间值

>   //
    // Map functions return a slice of KeyValue.
    //
    type KeyValue struct {
        Key   string
        Value string
    }

所以我们也要定义一个返回值
> kva := []mr.KeyValue{}

遍历刚才生成的words，因为我们只需要Word，不需要他的索引，因此第一个参数直接忽略，第二个参数用w标识
> for _, w := range words {
		kv := mr.KeyValue{w, "1"}
		kva = append(kva, kv)
	}

对每个word都生成一个KeyValue组合，这里并没有排除重复的情况，重复的情况要在后面reduce时候处理

> return kva
函数结尾返回所有的KeyValue组合，也就是我们所说的intermediate value

接下来再看一下reduce函数

>   // The reduce function is called once for each key generated by the
    // map tasks, with a list of all the values created for that key by
    // any map task.
    func Reduce(key string, values []string) string {
        // return the number of occurrences of this word.
        return strconv.Itoa(len(values))
    }
上文提到，Map函数会产生一个中间值，但是很明显，这个中间值会有冗余。我们需要对每一个KeyValue键值对
进行处理。这里对每一个KEY，我们会将其对应的string数组长度设置为一个字符串，然后返回。比如，对于
某一个Key "soccer" 来说，有77个"1"为对应的value，那我们就通过reduce函数产生一个"77"的字符串并且返回

接下来再看一下mrsequential.go这个文件

> mapf, reducef := loadPlugin(os.Args[1])
>
>   // load the application Map and Reduce functions
    // from a plugin file, e.g. ../mrapps/wc.so
    func loadPlugin(filename string) (func(string, string) []mr.KeyValue, func(string, []string) string) {
        p, err := plugin.Open(filename)
        if err != nil {
            log.Fatalf("cannot load plugin %v", filename)
        }
        xmapf, err := p.Lookup("Map")
        if err != nil {
            log.Fatalf("cannot find Map in %v", filename)
        }
        mapf := xmapf.(func(string, string) []mr.KeyValue)
        xreducef, err := p.Lookup("Reduce")
        if err != nil {
            log.Fatalf("cannot find Reduce in %v", filename)
        }
        reducef := xreducef.(func(string, []string) string)

        return mapf, reducef
    }


函数一开始调用了loadPlugin这个函数，这里其实就是将我们刚刚讲解的wc.go文件变成参数传入进去，Map和Reduce函数就成功地被导入进去。loadPlugin函数起到一个导入用户自己编写的Map和Reduce函数，并将它们分别命名为mapf, reducef的作用。

函数的下一步就是针对用户给定的所有文件，依次遍历一遍，并且调用Map函数生成我们需要的中间值
>   //
	// read each input file,
	// pass it to Map,
	// accumulate the intermediate Map output.
	//
	intermediate := []mr.KeyValue{}
	for _, filename := range os.Args[2:] {
		file, err := os.Open(filename)
		if err != nil {
			log.Fatalf("cannot open %v", filename)
		}
		content, err := ioutil.ReadAll(file)
		if err != nil {
			log.Fatalf("cannot read %v", filename)
		}
		file.Close()
		kva := mapf(filename, string(content))
		intermediate = append(intermediate, kva...)
	}
前面已经提到，wc.go中Map函数会返回一个KeyValue的列表，那么针对所有文件，我们可以得到一个包含所有wor的大列表，这个大列表就是我们所需要的中间值。

> sort.Sort(ByKey(intermediate))

调用排序功能，对大列表进行排序

接下来就是reduce函数的作用了。我们需要对这个大列表进行精简处理。

>   //
	// call Reduce on each distinct key in intermediate[],
	// and print the result to mr-out-0.
	//
	i := 0
	for i < len(intermediate) {
		j := i + 1
        // 因为已经排序好了，所以相同的Key必定在一起。这里统计一下对应的长度
		for j < len(intermediate) && intermediate[j].Key == intermediate[i].Key {
			j++
		}

        // 对每个KeyValue对，都需要生成一个string[]切片，具体内容均为"1"
		values := []string{}
		for k := i; k < j; k++ {
			values = append(values, intermediate[k].Value)
		}
        // 统计一下切片有多长
		output := reducef(intermediate[i].Key, values)

		// this is the correct format for each line of Reduce output.
        // 把结果添加到文件里
		fmt.Fprintf(ofile, "%v %v\n", intermediate[i].Key, output)

		i = j
	}

个人认为这里稍微有点拖沓，不过也不妨碍我们进行理解，总之单机版的wc就是这样实现的

接下来再看一下实验要求：
> We have given you a little code to start you off. The "main" routines for the coordinator and worker are in main/mrcoordinator.go and main/mrworker.go; don't change these files. 

这里mrcoodinator.go以及mrworker.go的主要作用都是调用我们自己实的coordinator和worker，所以接下来着重看一下这几个文件的代码

> You should put your implementation in mr/coordinator.go, mr/worker.go, and mr/rpc.go.

先看mr/worker.go
题目给出了提示： 
> One way to get started is to modify mr/worker.go's Worker() to send an RPC to the coordinator asking for a task. Then modify the coordinator to respond with the file name of an as-yet-unstarted map task. Then modify the worker to read that file and call the application Map function, as in mrsequential.go.
我们接下来主要的目标就是为worker实现一个对Coordinator发送请求任务的功能。

一个很自然的想法，既然Worker要向coordinator请求task,那么我们是否应该先定义一个task出来呢？而且我们的coordinator也应该拥有创建task的能力。

> mr/coordinator.go

> // 这里我们先定义一个乞丐版的task
> type Task struct {
	taskName string
}

同时，我们可以在rpc.go文件中定义一个专门为worker设计的GetTaskRequest
> // this rpc is for workers to get task from coordinator
type GetTaskRequest struct {
	Worker int
}

这里我觉得可以先把这个rpc对应worker信息加到这个Request里面

对应地，我们应该定义一个coordinator给Worker的回复

> // this response is for coordinators to return task to the worker
type GetTaskResponse struct {
	TaskName string
}

由于还不清楚具体应该怎么给worker回复，这两个rpc的内容我们暂时不管
相应地，在coordinator.go里面，我们可以创建处理GetTaskResponse的函数

> func (c *Coordinator) HandleGetTaskRequest(args *GetTaskRequest, reply *GetTaskResponse) error {
	reply.TaskName = c.task.taskName
	return nil
}

这里我们照猫画虎，借鉴worker.go文件中的代码，实现一个worker向coordinator发送请求task的rpc的功能
> func CallGetTask() {

	// declare an argument structure.
	args := GetTaskRequest{}

	// fill in the argument(s).
	args.Worker = 99

	// declare a reply structure.
	reply := GetTaskResponse{}

	// send the RPC request, wait for the reply.
	// the "Coordinator.HandleGetTaskRequest" tells the
	// receiving server that we'd like to call
	// the HandleGetTaskRequest method of struct Coordinator.
	ok := call("Coordinator.HandleGetTaskRequest", &args, &reply)
	if ok {
		// reply.Y should be 100.
		fmt.Printf("reply.Y %v\n", reply.TaskName)
	} else {
		fmt.Printf("call failed!\n")
	}
}

这段代码的大意就是，worker段构造一个GetTaskRequest{}和GetTaskResponse{}，通过Rpc调用coordinator的HandleGetTaskRequest函数，如果调用成功，就返回这个reply里面包含的TaskName

这里我们还需要给coordinator初始化一下TaskName

> c := Coordinator{task: Task{taskName: "Argentina"} }

也别忘记在worker.go里面调用CallGetTask()函数

> (base) main % bash test-mr.sh
*** Starting wc test.
reply.Y Argentina
reply.Y Argentina
reply.Y Argentina

启动测试脚本，发现coordinator可以正确处理来自Worker的rpc GetTaskRequest请求

我们在这里完成了HINT中的第一部分

接着我们来看一下worker部分

我们先试图实现一种最傻瓜的worker工作流程
1. 向Coordinator发送GetTask的请求
2. 收到Coordinator的回答，执行任务，这里的任务类型是Map还是Reduce取决于Coordinator目前所处的状态
3. 执行任务完毕后，向Coordinator返回执行结果

回到任务本身，Coordinator有可能向worker发送Map任务，也有可能发送Reduce任务。当Coordinator全部的任务都顺利完成后，此时若Worker继续请求任务，我们可以让Coordinator发送一种特殊的任务，这种任务就是让Worker自己退出，因此我们定义taskType和Task

> // define what kind of task it is
    type TaskType int

    const (
        _ TaskType = iota // 0 is unknown
        MapTask // 1: this task is Map task
        ReduceTask // 2: this task is reduce task
        AllTasksDone // 3: all of the tasks are finished, the worker should exit
    )

对于Coordinator来说，需要针对所有的任务进行维护。一个任务有未分配，正在运行，已完成三种情况，因此我们定义任务情况

> type TaskState int

const (
    Idle TaskState = iota
    InProgress
    Completed
)

在这里，我们希望定义一个Task. Task应该具有以下几个信息：
1. 这个Task是什么类型的Task? Map or Reduce?
2. 这个task需要处理哪些文件？如果是Map任务，应该处理几个文件？如果是Reduce任务，应该处理几个文件？
3. 这个任务是第几个任务？假设Coordinator需要处理30个任务，那么这个任务是第几个
4. 任务分配的时间。若超时我们会认为这个任务需要重新被分配
5. 任务状态，这个变量和上一个信息相呼应
6. NReduce。 根据论文，Worker需要在Map的时候将生成的中间值分为NReduce个文件，这些中间文件将用于后续的Reduce任务

这里我们给出对应的Task结构代码

> type Task struct {
    Type TaskType
	FileNames []string
	Id int
	StartTime int64
	State TaskState
	NReduce int
}

有了Task的定义，一个很自然的想法就是先实现Coordinator对于Task的分配。首先，我们需要让coordinator能够分配Map任务给worker。coordinator会维护一个任务列表，每个任务有一个状态（Idle、InProgress、Completed），以及任务的相关信息（如输入文件名、任务ID等）。接着我们定义一个Coordonaotr

> type Coordinator struct {
    mu       sync.Mutex
    mapTasks []Task
    // ... others
}

好的，我们现在为Coordinator分配了很多Task，这里我们先暂定所有的Task全是Map类型的。借用上一节的HandleGetTaskRequest方法：

> func (c *Coordinator) HandleGetTaskRequest(args *GetTaskRequest, reply *GetTaskResponse) error {
    c.mu.Lock()
    defer c.mu.Unlock()

    if c.phase == duringMap {
        // 初始状态
		reply.Task = findAvailableTask(c.mapTasks)
	} else if c.phase == duringReduce {
		// 所有Map任务执行完毕后，开始分配reduce任务
	} else {
        // 所有Reduce任务执行完毕后，让所有发出请求的Worker休眠
		// reply.Task = &Task{TaskType: AllTasksDone}
	}

    return nil
}

这里可以看到，Coordinator调用HandleGetTaskRequest方法来应对来自Worker方的请求。一开始我们默认Coordinator是需要先处理Map任务，这里我们调用findAvailableTask

> func findAvailableTask(tasks []*Task) *Task {
	for _, task := range tasks {
		if task.State == Idle || (task.State == InProgress && time.Since(task.StartTime) > TaskTimeout) {
			task.StartTime = time.Now()
			task.State = InProgress
			return task
		}
	}
	return nil
}

调用结束后，worker得到了 Task，需要对这个Task进行操作，这里我们知道这个Task是Map任务，所以我们对Map任务进行处理

> func HandleMapTask(task *Task, mapf func(string, string) []KeyValue) {
>
> // 1. Read input file
> // Worker的第一步就是将Coordinator给他传过来的文件名读取
> // 因为Map只需要将一个文件读取并分成NReduce个临时文件，所以我们这里只需要用FileNames[0]
	// 1. Read input file
	// since it is Map task, we only need to handle one file
    // 打开需要被Map的文件
	file, err := os.Open(task.FileNames[0])
	if err != nil {
		log.Fatalf("cannot open %v", task.FileNames[0])
	}
    // 读取文件内容
	content, err := ioutil.ReadAll(file)
	if err != nil {
		log.Fatalf("cannot read %v", task.FileNames[0])
	}
	file.Close()

	// 2. use our predefined map function to process the file
	kvs := mapf(task.FileNames[0], string(content))

	// 3. write the kv pairs into NReduce files
    // 这里我们定义一个临时的文件夹用来存放我们创建好的中间值文件
	intermediateDir := "./intermediates"
	err = os.MkdirAll(intermediateDir, os.ModePerm)
	if err != nil {
		log.Fatalf("cannot create directory %v", intermediateDir)
	}
	// here we need to split the file into NReduce number of files
    // 记得这里，每一个文件，我们都要把它分为NReduce个中间值文件
	bucketfiles := make([]*os.File, task.NReduce)
	for i := 0; i < task.NReduce; i++ {
        // 这里先用starttime来确保一下当前这个文件是唯一的
		intermediateFile := fmt.Sprintf("%s/mr-%d-%d.tmp%v",
			intermediateDir,
			task.Id,
			i,
			task.StartTime.UnixNano())
		f, err := os.OpenFile(intermediateFile, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, 0666)
		if err != nil {
			log.Fatalf("cannot create intermediate file %v", intermediateFile)
		}
		bucketfiles[i] = f
		defer f.Close()
	}

	// write the kv pairs into NReduce files
    // 在生成文件之后，我们需要把map好的数据写入这些文件中
	for _, kv := range kvs {
		// using ihash function to find out which file the key value pair should be written into
        // 这里需要用到ihash方法
		idx := ihash(kv.Key) % task.NReduce
		_, err := bucketfiles[idx].WriteString(fmt.Sprintf("%s %s\n", kv.Key, kv.Value))
		if err != nil {
			panic(fmt.Sprintf("worker crash on map task, fail append kv (%s, %s) to intermediate file %s", kv.Key, kv.Value, bucketfiles[idx].Name()))
		}
	}
	// Close all files
	for _, file := range bucketfiles {
		file.Close()
	}
    // Rename the temporary files
	for _, file := range bucketfiles {
		originalName := file.Name()
		finalName := strings.TrimSuffix(originalName, filepath.Ext(originalName))
		err := os.Rename(originalName, finalName)
		if err != nil {
			log.Fatalf("cannot rename intermediate file %v to %v", originalName, finalName)
		}
	}

经过对Map task处理函数的编写，我们可以通过调用worker和coordinator通信来生成中间值文件了。注意我们这里并没有完全写完这个函数，也没有正确处理完临时文件。经过测试，coordinator分配任务，Worker可以生成中间文件，接下来我们要实现worker对coordinator的答复，coordinator对应的处理

// 插入图片

在这里，注意到，Worker在完成这个Map任务的时候，应该给Coordinator发送一条消息，Coordinator再根据worker的反馈来作出相应的变化。如果完成，那就需要将这个任务变成Completed，如果没完成，那就继续发给别的Worker。

这里，继续定义一种由worker调用的Rpc

> // this request is for worker to tell the coordinator that it has finished
// the task it was assigned to
type TaskDoneOrNotRequest struct {
	Task *Task
}

type TaskDoneOrNotReply struct {
	// coordinator does not reply
}

在Coordinator中添加一个taskdone函数，方便worker调用

> func (c *Coordinator) TaskDone(args *TaskDoneOrNotRequest, reply *TaskDoneOrNotReply) error {
    c.mu.Lock()
    defer c.mu.Unlock()

    // Mark the task as completed
    for _, task := range c.mapTasks {
        if task.Id == args.Task.Id {
            task.State = Completed
			log.Printf("Coordinator: Task completed: %+v\n", task) // Print the task completion
            break
        }
    }
}

这里稍显啰嗦，不过我们可以之后再继续进行优化

在worker中添加一个函数，完成map任务后调用，使得worker可以调用coordinator的task done函数

> func notifyTaskDone(task *Task) {
	args := TaskDoneOrNotRequest{Task: task}
	reply := TaskDoneOrNotReply{}

	ok := call("Coordinator.TaskDone", &args, &reply)
	if ok {
		fmt.Printf("Worker: Notified task done: %+v\n", task)
	} else {
		fmt.Printf("Worker: Failed to notify task done: %+v\n", task)
	}
}

别忘记调用这个函数

> defer func() {
		if err := recover(); err != nil {
			fmt.Printf("worker: doMap panic, %s", err)
			task.State = Idle // 通知 Coordinator 本次任务执行失败
		} else {
			task.State = Completed
			notifyTaskDone(task)
		}
	}()

经过测试，我发现coordinator这里正确地收到了来自worker的调用，我这里把调试的结果放出来

> *** Starting wc test.
	2024/07/16 14:02:57 Coordinator: Assigning task: &{Type:1 FileNames:[../pg-being_ernest.txt] Id:0 StartTime:2024-07-16 14:02:57.363389 -0700 PDT m=+1.007003584 State:1 NReduce:10}
	2024/07/16 14:02:57 Coordinator: Assigning task: &{Type:1 FileNames:[../pg-dorian_gray.txt] Id:1 StartTime:2024-07-16 14:02:57.363827 -0700 PDT m=+1.007441001 State:1 NReduce:10}
	2024/07/16 14:02:57 Worker: Received task: &{Type:1 FileNames:[../pg-being_ernest.txt] Id:0 StartTime:2024-07-16 14:02:57.363389 -0700 PDT State:1 NReduce:10}
	2024/07/16 14:02:57 Worker: Received task: &{Type:1 FileNames:[../pg-dorian_gray.txt] Id:1 StartTime:2024-07-16 14:02:57.363827 -0700 PDT State:1 NReduce:10}
	2024/07/16 14:02:57 Coordinator: Assigning task: &{Type:1 FileNames:[../pg-frankenstein.txt] Id:2 StartTime:2024-07-16 14:02:57.365043 -0700 PDT m=+1.008657626 State:1 NReduce:10}
	2024/07/16 14:02:57 Worker: Received task: &{Type:1 FileNames:[../pg-frankenstein.txt] Id:2 StartTime:2024-07-16 14:02:57.365043 -0700 PDT State:1 NReduce:10}
	2024/07/16 14:02:57 Coordinator: Task completed: &{Type:1 FileNames:[../pg-being_ernest.txt] Id:0 StartTime:2024-07-16 14:02:57.363389 -0700 PDT m=+1.007003584 State:2 NReduce:10}
	Worker: Notified task done: &{Type:1 FileNames:[../pg-being_ernest.txt] Id:0 StartTime:2024-07-16 14:02:57.363389 -0700 PDT State:2 NReduce:10}
	2024/07/16 14:02:57 Coordinator: Task completed: &{Type:1 FileNames:[../pg-frankenstein.txt] Id:2 StartTime:2024-07-16 14:02:57.365043 -0700 PDT m=+1.008657626 State:2 NReduce:10}
	Worker: Notified task done: &{Type:1 FileNames:[../pg-frankenstein.txt] Id:2 StartTime:2024-07-16 14:02:57.365043 -0700 PDT State:2 NReduce:10}
	2024/07/16 14:02:57 Coordinator: Task completed: &{Type:1 FileNames:[../pg-dorian_gray.txt] Id:1 StartTime:2024-07-16 14:02:57.363827 -0700 PDT m=+1.007441001 State:2 NReduce:10}
	Worker: Notified task done: &{Type:1 FileNames:[../pg-dorian_gray.txt] Id:1 StartTime:2024-07-16 14:02:57.363827 -0700 PDT State:2 NReduce:10}

这里我们可以看到，有3个worker都分别执行了一次map任务后就停止了，因此我们希望能实现单个worker循环完成任务。在修改worker代码的时候，我发现之前的CallGetTaskRequest函数有些不清晰，重写一下：

> func CallGetTask() *Task{

	// declare an argument structure.
	args := GetTaskRequest{}

	// fill in the argument(s).
	args.Worker = 99

	// declare a reply structure.
	reply := GetTaskResponse{}

	// send the RPC request, wait for the reply.
	// the "Coordinator.HandleGetTaskRequest" tells the
	// receiving server that we'd like to call
	// the HandleGetTaskRequest method of struct Coordinator.
	ok := call("Coordinator.HandleGetTaskRequest", &args, &reply)
	if ok {
		// log.Printf("Worker: Received task: %+v\n", reply.Task)
		if reply.Task == nil {
			// No more tasks
			return nil
		}
		log.Printf("Worker: Received task: %+v\n", reply.Task)
		return reply.Task
	} else {
		fmt.Printf("CallGetTask failed!\n")
		return nil
	}
}

在得到task之后，我们给worker外面套一个循环，使得worker可以循环处理来自coordinator的任务

> //
// main/mrworker.go calls this function.
//
func Worker(mapf func(string, string) []KeyValue,
	reducef func(string, []string) string) {

	// Your worker implementation here.

	// uncomment to send the Example RPC to the coordinator.
	// CallExample()
	// CallGetTask(mapf)
	
	// use a for loop to repeatedly process all of the tasks
	for {

		task := CallGetTask()
		if task == nil {
			// if there is no task, the worker should wait for alltaskdone signal
			fmt.Println("task is nil, continue")
			time.Sleep(2 * time.Second)
			continue
		}

		fmt.Printf("worker: receive coordinators get task: %v\n", task)
		// now we have the task
		switch task.Type {
		case MapTask:
			HandleMapTask(task, mapf)
		case ReduceTask:
			
		case AllTasksDone:
			// all tasks finished, the worker should return
			return
		default:
			// not the task type we want
			fmt.Printf("unexpected TaskType %v", task.Type)
			time.Sleep(time.Second)
			continue
		}
	}

}

经过测试，我们发现八个文件都被处理成了map所生产的中间文件格式，此时发生了一件尴尬的事情，我没有办法手动关闭测试脚本，worker一直在等待任务，进入了死循环，这里我们可以之后给他加上一个定时器，让worker自动退出

在完成了map功能以后，我们接下来的一个重点就是完成reduce功能

同样的，这一任务的重点是写一个HandleReduceTaskRequest的函数。在写这个函数之前。我们可以先试试，当所有map tasks都被完成以后，coorinator的状态是否如我们所需要的那样转换为reduce状态。这里我们在判断所有map任务结束后，需要手动将状态改一下，然后生成reduce任务

> // Add logic to move to the next phase if all map tasks are completed
    allMapTasksCompleted := true
    for _, task := range c.mapTasks {
        if task.State != Completed {
            allMapTasksCompleted = false
            break
        }
    }

    if allMapTasksCompleted {
        // Move to the reduce phase
        c.phase = duringReduce
		log.Printf("Coordinator: all map tasks completed, change to next phase: %v\n", c.phase)
        c.GenerateReduceTasksWithLock()
		// this function is for debugging
        c.PrintReduceTasksWithLock()
	}

> // GenerateReduceTasks generates the Reduce tasks after all Map tasks are completed.
func (c *Coordinator) GenerateReduceTasksWithLock() {
	// c.mu.Lock()
	// defer c.mu.Unlock()

	if c.phase != duringReduce {
		return
	}

	// Create Reduce tasks
	for i := 0; i < c.nReduce; i++ {
		c.reduceTasks = append(c.reduceTasks, &Task{
			Type:      ReduceTask,
			Id:        i,
			FileNames: nil, // Will be filled later
			NMap:      len(c.mapTasks),
			NReduce:   c.nReduce,
			State:     Idle,
		})
	}

	c.phase = duringReduce
}
这里注意，创建任务的时候外面的函数已经加了锁，我们不需要加锁，否则会有死锁
经过测试，在完成了所有的Map任务后，reduce任务被成功创建，那么我们下一步就是将filenames填进去

> // AssignReduceTasks assigns the files for each Reduce task.
func (c *Coordinator) AssignReduceTasksWithLock() {

	// Assuming intermediate files are named as "mr-x-y.tmp"
	for i := 0; i < c.nReduce; i++ {
		task := c.reduceTasks[i]
		for j := 0; j < len(c.mapTasks); j++ {
			intermediateFile := fmt.Sprintf("./intermediates/mr-%d-%d.tmp", j, i)
			task.FileNames = append(task.FileNames, intermediateFile)
		}
		task.State = Idle
	}
}

经过测试，我们确实可以得到对应的reduce任务
> 2024/07/16 16:51:20 Coordinator: all map tasks completed, change to next phase: 1
2024/07/16 16:51:20 Coordinator: Current reduce tasks: 
2024/07/16 16:51:20 Reduce Task ID: 0, State: 0, FileNames: [./intermediates/mr-0-0.tmp ./intermediates/mr-1-0.tmp ./intermediates/mr-2-0.tmp ./intermediates/mr-3-0.tmp ./intermediates/mr-4-0.tmp ./intermediates/mr-5-0.tmp ./intermediates/mr-6-0.tmp ./intermediates/mr-7-0.tmp]
2024/07/16 16:51:20 Reduce Task ID: 1, State: 0, FileNames: [./intermediates/mr-0-1.tmp ./intermediates/mr-1-1.tmp ./intermediates/mr-2-1.tmp ./intermediates/mr-3-1.tmp ./intermediates/mr-4-1.tmp ./intermediates/mr-5-1.tmp ./intermediates/mr-6-1.tmp ./intermediates/mr-7-1.tmp]
2024/07/16 16:51:20 Reduce Task ID: 2, State: 0, FileNames: [./intermediates/mr-0-2.tmp ./intermediates/mr-1-2.tmp ./intermediates/mr-2-2.tmp ./intermediates/mr-3-2.tmp ./intermediates/mr-4-2.tmp ./intermediates/mr-5-2.tmp ./intermediates/mr-6-2.tmp ./intermediates/mr-7-2.tmp]
2024/07/16 16:51:20 Reduce Task ID: 3, State: 0, FileNames: [./intermediates/mr-0-3.tmp ./intermediates/mr-1-3.tmp ./intermediates/mr-2-3.tmp ./intermediates/mr-3-3.tmp ./intermediates/mr-4-3.tmp ./intermediates/mr-5-3.tmp ./intermediates/mr-6-3.tmp ./intermediates/mr-7-3.tmp]
2024/07/16 16:51:20 Reduce Task ID: 4, State: 0, FileNames: [./intermediates/mr-0-4.tmp ./intermediates/mr-1-4.tmp ./intermediates/mr-2-4.tmp ./intermediates/mr-3-4.tmp ./intermediates/mr-4-4.tmp ./intermediates/mr-5-4.tmp ./intermediates/mr-6-4.tmp ./intermediates/mr-7-4.tmp]
2024/07/16 16:51:20 Reduce Task ID: 5, State: 0, FileNames: [./intermediates/mr-0-5.tmp ./intermediates/mr-1-5.tmp ./intermediates/mr-2-5.tmp ./intermediates/mr-3-5.tmp ./intermediates/mr-4-5.tmp ./intermediates/mr-5-5.tmp ./intermediates/mr-6-5.tmp ./intermediates/mr-7-5.tmp]
2024/07/16 16:51:20 Reduce Task ID: 6, State: 0, FileNames: [./intermediates/mr-0-6.tmp ./intermediates/mr-1-6.tmp ./intermediates/mr-2-6.tmp ./intermediates/mr-3-6.tmp ./intermediates/mr-4-6.tmp ./intermediates/mr-5-6.tmp ./intermediates/mr-6-6.tmp ./intermediates/mr-7-6.tmp]
2024/07/16 16:51:20 Reduce Task ID: 7, State: 0, FileNames: [./intermediates/mr-0-7.tmp ./intermediates/mr-1-7.tmp ./intermediates/mr-2-7.tmp ./intermediates/mr-3-7.tmp ./intermediates/mr-4-7.tmp ./intermediates/mr-5-7.tmp ./intermediates/mr-6-7.tmp ./intermediates/mr-7-7.tmp]
2024/07/16 16:51:20 Reduce Task ID: 8, State: 0, FileNames: [./intermediates/mr-0-8.tmp ./intermediates/mr-1-8.tmp ./intermediates/mr-2-8.tmp ./intermediates/mr-3-8.tmp ./intermediates/mr-4-8.tmp ./intermediates/mr-5-8.tmp ./intermediates/mr-6-8.tmp ./intermediates/mr-7-8.tmp]
2024/07/16 16:51:20 Reduce Task ID: 9, State: 0, FileNames: [./intermediates/mr-0-9.tmp ./intermediates/mr-1-9.tmp ./intermediates/mr-2-9.tmp ./intermediates/mr-3-9.tmp ./intermediates/mr-4-9.tmp ./intermediates/mr-5-9.tmp ./intermediates/mr-6-9.tmp ./intermediates/mr-7-9.tmp]
Worker: Notified task done: &{Type:1 FileNames:[../pg-sherlock_holmes.txt] Id:6 StartTime:2024-07-16 16:51:19.691652 -0700 PDT State:2 NMap:0 NReduce:10}

可以观察到，每个reduce任务都负责处理8个中间文件，正好符合我们nReduce的要求

生成正确的reduce任务后，我们的下一步目标就是写一个worker处理reduce任务的函数

1. 读取我们map生成的中间值文件，解析键值对
要解析的文件已经在这个task对应的文件名切片中了，我们只需要遍历一遍即可
> var kvs []KeyValue
	for _, fileName := range task.FileNames {
		// first we have to open the file
		f, err := os.Open(fileName)
		if err != nil {
			panic(fmt.Sprintf("worker crash on reduce task, fail to open %s", fileName))
		}
		defer f.Close()

		// here we scan through each line of the txt file
		sc := bufio.NewScanner(f)
		for sc.Scan() {
			kvStrs := strings.Fields(sc.Text())
			if len(kvStrs) != 2 {
				panic("worker crash on reduce task, kv format wrong")
			}
			kvs = append(kvs, KeyValue{
				Key:   kvStrs[0],
				Value: kvStrs[1],
			})
		}

		// now we have all of the KeyValues in a slice, sort this slice according to key
		sort.Slice(kvs, func(i, j int) bool {
			return kvs[i].Key < kvs[j].Key
		})

经过上述处理，我们把所需要处理的文件遍历了一遍，并且对他们进行了排序

> i := 0 // start idx for same keys
		for i < len(kvs) {
			j := i + 1 // end idx for same keys
			for j < len(kvs) && kvs[j].Key == kvs[i].Key {
				j++
			}

			// collect values with the same key
			values := []string{}
			for k := i; k < j; k++ {
				values = append(values, kvs[k].Value)
			}
			output := reducef(kvs[i].Key, values)

			// this is the correct format for each line of Reduce output.
			fmt.Fprintf(ofile, "%v %v\n", kvs[i].Key, output)

			i = j
		}

这里我们借鉴了mrsequencial里面的方法

> // rename the file
		os.Rename(ofile.Name(), strings.TrimSuffix(ofile.Name(), fmt.Sprintf(".tmp%v", task.StartTime)))
重命名一下这个文件
因为题目要求我们不合并这些生成的文件，所以主要功能就实现的差不多了

经过测试，我发现，只有Crash test过不了，我在运行的时候看了一下，大概就是worker在运行的时候突然宕机，产生的临时文件没有被清理

目前来说只有最后一个crash test无法通过，待续。。。